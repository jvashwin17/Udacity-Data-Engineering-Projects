Project Overwiew
Create a high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. Data quality plays a big part when analyses are executed on top the data warehouse. The source data resides in S3 and processed in Sparkify's data warehouse in Amazon Redshift. 

Datasets:
Log data: s3://udacity-dend/log_data
Song data: s3://udacity-dend/song_data

Airflow DAG Execution Phases:
Begin Execution -> Stage events and songs -> Load songplay facts table -> Load song,time,user,artist dimension table -> Run dataquality checks -> End Execution

Schema
Fact Table songplays - records in log data associated with song plays i.e. records with page NextSong Table rows - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

Dimension Tables

users - users in the app Table rows - user_id, first_name, last_name, gender, level
songs - songs in music database Table Rows -song_id, title, artist_id, year, duration
artists - artists in music database Table rows - artist_id, name, location, latitude, longitude
time - timestamps of records in songplays broken down into specific units Table rows - start_time, hour, day, week, month, year, weekday

Project Data Files
Song Dataset The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. Sample Data: {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

Log Dataset The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. Sample Data : {"artist": null, "auth": "Logged In", "firstName": "Walter", "gender": "M", "itemInSession": 0, "lastName": "Frye", "length": null, "level": "free", "location": "San Francisco-Oakland-Hayward, CA", "method": "GET","page": "Home", "registration": 1540919166796.0, "sessionId": 38, "song": null, "status": 200, "ts": 1541105830796, "userAgent": ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36"", "userId": "39"}

Project Source Files
File Names	Description
dimenstion_subday.py	This file is used to load dimention table
udac_example_day.py - This is the main file used to execute the DAGs in correct order
create_tables.sql - This file has create tables queries for Facts and Dimesntion tables.
create_tables.py	This file is used to create new tables. 
load_dimension.py - This file is used load dimention tables.
load_fact.py - > This file is used to load fact tables. 
stage_readshift.py -> Redshift connection and BaseOperator details
data_quality.py -> This file is used to check the data quiality once all the tables are loaded with the data.
sql_queries.py	Contains all the Insert queries.

Technology Stack
Python 3.6 or above PostgresSQL 9.5 or above

Run the project
After you have updated the DAG, you will need to run /opt/airflow/start.sh command to start the Airflow web server. 
Once the Airflow web server is ready, you can access the Airflow UI by clicking on the blue Access Airflow button at the bottom.

